// This file is automatically generated by update_python.sh
// Do not edit manually

/// Python script content for clustering_connect.py
pub const CLUSTERING_CONNECT_SCRIPT: &str = r#"import pandas as pd
import numpy as np
import argparse
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering, MeanShift, SpectralClustering, Birch
from sklearn.mixture import GaussianMixture
from sklearn.metrics import silhouette_score
import json
import mmap

def perform_kmeans(X, n_clusters):
    kmeans = KMeans(n_clusters=n_clusters, random_state=0)
    return kmeans.fit_predict(X)

def perform_dbscan(X, eps, min_samples):
    dbscan = DBSCAN(eps=eps, min_samples=min_samples)
    return dbscan.fit_predict(X)

def perform_agglomerative(X, n_clusters):
    agglomerative = AgglomerativeClustering(n_clusters=n_clusters)
    return agglomerative.fit_predict(X)

def perform_mean_shift(X):
    mean_shift = MeanShift()
    return mean_shift.fit_predict(X)

def perform_gmm(X, n_clusters):
    gmm = GaussianMixture(n_components=n_clusters, random_state=0)
    gmm.fit(X)
    return gmm.predict(X)

def perform_spectral(X, n_clusters):
    spectral = SpectralClustering(n_clusters=n_clusters, random_state=0)
    return spectral.fit_predict(X)

def perform_birch(X, n_clusters):
    birch = Birch(n_clusters=n_clusters)
    return birch.fit_predict(X)

def find_optimal_clusters_elbow(X):
    wcss = []
    for i in range(1, 11):
        kmeans = KMeans(n_clusters=i, random_state=0)
        kmeans.fit(X)
        wcss.append(kmeans.inertia_)

    # Automatically determine the "elbow" point
    deltas = np.diff(wcss)
    second_deltas = np.diff(deltas)
    optimal_clusters = np.argmax(second_deltas) + 2  # +2 to offset the second difference
    return optimal_clusters

def find_optimal_clusters_silhouette(X):
    silhouette_scores = []
    for i in range(2, min(11, len(X))):
        kmeans = KMeans(n_clusters=i, random_state=0)
        labels = kmeans.fit_predict(X)
        if len(np.unique(labels)) > 1:
            silhouette_scores.append(silhouette_score(X, labels))
        else:
            silhouette_scores.append(-1)  # Invalid score if only one cluster

    # Automatically determine the optimal number of clusters
    optimal_clusters = np.argmax(silhouette_scores) + 2  # +2 because range starts from 2
    return optimal_clusters

def parse_optimal_clustering_method(optimal_clustering_method):
    if optimal_clustering_method.startswith('FIXED:'):
        return int(optimal_clustering_method.split(':')[1])
    elif optimal_clustering_method in ['ELBOW', 'SILHOUETTE']:
        return optimal_clustering_method
    else:
        raise ValueError("Unsupported optimal clustering method. Choose from 'FIXED:n', 'ELBOW' or 'SILHOUETTE'.")

def main(uid, csv_path, features, operation, cluster_column_name, optimal_n_cluster_finding_method, dbscan_eps, dbscan_min_samples):
    # Load the CSV file
    data = pd.read_csv(csv_path)

    # Select the features for clustering
    feature_list = [feature.strip() for feature in features.split(',')]
    X = data[feature_list]

    # Determine the optimal number of clusters if required
    n_clusters = None
    if optimal_n_cluster_finding_method and operation != 'DBSCAN':
        optimal_method = parse_optimal_clustering_method(optimal_n_cluster_finding_method)
        if isinstance(optimal_method, int):
            n_clusters = optimal_method
        elif optimal_method == 'ELBOW':
            n_clusters = find_optimal_clusters_elbow(X)
        elif optimal_method == 'SILHOUETTE':
            n_clusters = find_optimal_clusters_silhouette(X)
    elif operation != 'DBSCAN' and not optimal_clustering_method:
        raise ValueError("Optimal clustering method must be specified for algorithms that require n_clusters")

    # Perform the chosen clustering operation
    if operation == 'KMEANS':
        data[cluster_column_name] = perform_kmeans(X, n_clusters)
    elif operation == 'DBSCAN':
        data[cluster_column_name] = perform_dbscan(X, dbscan_eps, dbscan_min_samples)
    elif operation == 'AGGLOMERATIVE':
        data[cluster_column_name] = perform_agglomerative(X, n_clusters)
    elif operation == 'MEAN_SHIFT':
        data[cluster_column_name] = perform_mean_shift(X)
    elif operation == 'GMM':
        data[cluster_column_name] = perform_gmm(X, n_clusters)
    elif operation == 'SPECTRAL':
        data[cluster_column_name] = perform_spectral(X, n_clusters)
    elif operation == 'BIRCH':
        data[cluster_column_name] = perform_birch(X, n_clusters)
    else:
        raise ValueError("Operation must be one of 'KMEANS', 'DBSCAN', 'AGGLOMERATIVE', 'MEAN_SHIFT', 'GMM', 'SPECTRAL', or 'BIRCH'")

    # Print the resulting clusters
    #print(data)

    # Prepare the final output
    headers = list(data.columns)
    rows = data.values.tolist()
    if operation in ['DBSCAN', 'MEAN_SHIFT']:
        report = f'Clustering performed using {operation}'
    else:
        report = f'Clustering performed using {operation} with {n_clusters} clusters'

    output = {
        "headers": headers,
        "rows": [[str(item) for item in row] for row in rows],
        "report": report
    }

    #print(json.dumps(output, indent=4))
    """
    json_output = json.dumps(output, indent=4)
    with open('output.json', 'wb') as f:
        # Resize the file to the size of the JSON output
        f.write(b' ' * len(json_output))

    with open('output.json', 'r+b') as f:
        mm = mmap.mmap(f.fileno(), 0)
        mm.write(json_output.encode('utf-8'))
        mm.close()
    """

    json_output = json.dumps(output, indent=4)
    filename = f"rgwml_{uid}.json"
    with open(filename, 'wb') as f:
        # Resize the file to the size of the JSON output
        f.write(b' ' * len(json_output))

    with open(filename, 'r+b') as f:
        mm = mmap.mmap(f.fileno(), 0)
        mm.write(json_output.encode('utf-8'))
        mm.close()




if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Cluster customers using various clustering algorithms.')
    
    parser.add_argument('--uid', type=str, help='A unique identifier to name the output json file', required=True)

    parser.add_argument('--csv_path', type=str, required=True, help='Path to the CSV file.')
    parser.add_argument('--features', type=str, required=True, help='Comma-separated list of features to use for clustering.')
    parser.add_argument('--cluster_column_name', type=str, required=True, help='Name of the clustering column.')
    parser.add_argument('--operation', type=str, required=True, choices=['KMEANS', 'DBSCAN', 'AGGLOMERATIVE', 'MEAN_SHIFT', 'GMM', 'SPECTRAL', 'BIRCH'], help='Clustering operation to perform.')
    # optimal_n_cluster_finding_method is not relevant for MEAN_SHIFT and DBSCAN
    parser.add_argument('--optimal_n_cluster_finding_method', type=str, help='Method to find the optimal number of clusters. Options: FIXED:n, ELBOW, SILHOUETTE. This value is not relevant for MEAN_SHIFT and DBSCAN operations')
    # dbscan_eps and dbscan_min_samples are only relevant for DBSCAN
    parser.add_argument('--dbscan_eps', type=float, default=0.5, help='The maximum distance between two samples for one to be considered as in the neighborhood of the other (used in DBSCAN). This value is only relevant for DBSCAN operations')
    parser.add_argument('--dbscan_min_samples', type=int, default=5, help='The number of samples (or total weight) in a neighborhood for a point to be considered as a core point (used in DBSCAN). This value is only relevant for DBSCAN operations')

    args = parser.parse_args()

    main(args.uid, args.csv_path, args.features, args.operation, args.cluster_column_name, args.optimal_n_cluster_finding_method, args.dbscan_eps, args.dbscan_min_samples)"#;

/// Python script content for dask_cleaner_connect.py
pub const DASK_CLEANER_CONNECT_SCRIPT: &str = r#"import argparse
import dask.dataframe as dd
import pandas as pd
import json
from collections import defaultdict
from datetime import datetime
import heapq
import mmap

def validate_value(value, rule):
    if pd.isna(value):
        return False
    
    if rule == "IS_NUMERICAL_VALUE":
        return value.replace('.', '', 1).isdigit()
    elif rule == "IS_POSITIVE_NUMERICAL_VALUE":
        return value.replace('.', '', 1).isdigit() and float(value) > 0
    elif rule.startswith("IS_LENGTH:"):
        length = int(rule.split(":")[1])
        return len(value) == length
    elif rule.startswith("IS_MIN_LENGTH:"):
        min_length = int(rule.split(":")[1])
        return len(value) >= min_length
    elif rule.startswith("IS_MAX_LENGTH:"):
        max_length = int(rule.split(":")[1])
        return len(value) <= max_length
    elif rule == "IS_VALID_TEN_DIGIT_INDIAN_MOBILE_NUMBER":
        return len(value) == 10 and value.isdigit() and value[0] in "6789" and len(set(value)) >= 3
    elif rule == "IS_NOT_AN_EMPTY_STRING":
        return bool(value)
    elif rule == "IS_DATETIME_PARSEABLE":
        formats = [
            "%Y-%m-%d", "%Y-%m-%d %H:%M:%S", "%Y/%m/%d",
            "%d-%m-%Y", "%Y-%m-%d %H:%M:%S.%f", "%b %d, %Y"
        ]
        for fmt in formats:
            try:
                datetime.strptime(value, fmt)
                return True
            except ValueError:
                continue
        return False
    else:
        return False

def validate_row(row, column_rules, non_compliant_counts):
    is_compliant = True
    for col, rule_list in column_rules.items():
        for rule in rule_list:
            if not validate_value(row[col], rule):
                non_compliant_counts[col][row[col]] += 1
                is_compliant = False
                break  # Stop checking other rules for this column
    return is_compliant

def process_csv_with_dask(csv_path, rules, action, show_unclean_examples_in_report):
    # Read CSV file into a Dask DataFrame
    df = dd.read_csv(csv_path, dtype=str, blocksize=25e6)  # Adjust blocksize as necessary
    #print(df)

    """
    # Count the number of NaN/None values in the 'mobile' column
    nan_count = df['mobile'].isna().sum().compute()
    # Print the result
    print(f"Number of NaN/None values in the 'mobile' column: {nan_count}")

    # Count the number of values in the 'mobile' column that have 10 digits and start with either 6, 7, 8, or 9
    valid_mobile_count = df['mobile'].str.match(r'^[6789]\d{9}$').sum().compute()
    print(f"Number of valid 10-digit mobile numbers starting with 6, 7, 8, or 9: {valid_mobile_count}")

    # Count the number of values in the 'days_since_first_payment' column that have a length of 2 digits
    two_digit_days_count = df['days_since_first_payment'].astype(str).str.len().eq(2).sum().compute()
    print(f"Number of values in the 'days_since_first_payment' column with a length of 2 digits: {two_digit_days_count}")
    """

    column_rules = {col: rule_list for col, rule_list in rules}

    non_compliant_counts = defaultdict(lambda: defaultdict(int))

    # Validate each row and add a compliance column
    df['is_compliant'] = df.map_partitions(lambda df_part: df_part.apply(lambda row: validate_row(row, column_rules, non_compliant_counts), axis=1))

    # Compute the results
    results = df.compute()
    initial_total_rows = len(results)

    # Filter out non-compliant rows
    results = results[results['is_compliant']].drop(columns=['is_compliant']).reset_index(drop=True)
    #print(results)
    total_rows_after_cleaning = len(results)

    report = None
    if action != 'CLEAN':
        # Generate the cleanliness report
        report = {
            "total_rows": str(initial_total_rows),
            "total_rows_after_cleaning": str(total_rows_after_cleaning),
            "percentage_row_reduction_after_cleaning": f"{round((initial_total_rows - total_rows_after_cleaning) / initial_total_rows * 100, 2):.2f}",
            "column_cleanliness_analysis": {}
        }
        
        column_analysis = {}
        NAN_CONDITION_TRIGGERED = False  # Initialize the flag

        for col, values in non_compliant_counts.items():
            rules = column_rules[col]
            total_unique_values = len(values) - 2
            total_non_compliant = sum(values.values()) - 2
            percentage_non_compliant = (total_non_compliant / initial_total_rows) * 100

            analysis = {
                "rules": rules,
                "total_unique_values": str(total_unique_values),
                "total_non_compliant": str(total_non_compliant),
                "percentage_non_compliant": f"{percentage_non_compliant:.2f}",
            }

            if show_unclean_examples_in_report:
                top_unclean_values = heapq.nlargest(10, values.items(), key=lambda x: x[1])
                top_unclean_values_filtered = {str(value): count for value, count in top_unclean_values if pd.notna(value) and value != 'a'}
                analysis["top_10_max_freq_unclean_values"] = top_unclean_values_filtered
                if total_non_compliant > 0 and top_unclean_values_filtered == {}:
                    analysis["top_10_max_freq_unclean_values"] = { "": total_non_compliant }

            column_analysis[col] = analysis
            report["column_cleanliness_analysis"][col] = analysis

        report["column_cleanliness_analysis"] = column_analysis

        # Add the report to your output or log it as needed
        #print(report)


    if action == 'ANALYZE':
        json_report = json.dumps({"report": report}, indent=4)
        #print(json_report)
        return {"report": report}
    elif action == 'CLEAN':
        headers = list(results.columns)
        rows = results.fillna('').values.tolist()
        clean_data = {
            "headers": headers,
            "rows": [[str(item) for item in row] for row in rows]
        }
        json_data = json.dumps(clean_data, indent=4)
        #print(json_data)
        return clean_data
    elif action == 'ANALYZE_AND_CLEAN':
        headers = list(results.columns)
        rows = results.fillna('').values.tolist()
        clean_data_with_report = {
            "headers": headers,
            "rows": [[str(item) for item in row] for row in rows],
            "report": report
        }
        json_data_with_report = json.dumps(clean_data_with_report, indent=4)
        #print(report)
        return clean_data_with_report

def parse_rules(rules_str):
    rules = []
    for rule_pair in rules_str.split(';'):
        column, *rule_list = rule_pair.split(':')
        rule_list = ':'.join(rule_list).split(',')
        rules.append((column.strip(), rule_list))
    return rules

def main():
    parser = argparse.ArgumentParser(description='Validate CSV columns based on specified rules and generate a cleanliness report using Dask')

    parser.add_argument('--uid', type=str, help='A unique identifier to name the output json file', required=True)

    parser.add_argument('--path', type=str, required=True, help='Path to the input CSV file')
    parser.add_argument('--rules', type=str, required=True, help='Validation rules in the format "Column1:Rule1,Rule2;Column2:Rule3,Rule4"')
    parser.add_argument('--action', type=str, required=True, choices=['CLEAN', 'ANALYZE', 'ANALYZE_AND_CLEAN'], help='Action to perform: CLEAN, ANALYZE, or ANALYZE_AND_CLEAN')
    parser.add_argument('--show_unclean_values_in_report', type=str, choices=['TRUE', 'FALSE'], default='FALSE', help='Flag to show unclean values in the report')

    args = parser.parse_args()
    rules = parse_rules(args.rules)
    show_unclean_values_in_report = args.show_unclean_values_in_report == 'TRUE'

    json_output = process_csv_with_dask(args.path, rules, args.action, show_unclean_values_in_report)
    # You can now use json_result as needed in your script
    #print(json.dumps(json_result, indent=4))

    json_output = json.dumps(json_output, indent=4)
    """
    with open('output.json', 'wb') as f:
        # Resize the file to the size of the JSON output
        f.write(b' ' * len(json_output))

    with open('output.json', 'r+b') as f:
        mm = mmap.mmap(f.fileno(), 0)
        mm.write(json_output.encode('utf-8'))
        mm.close()
    """
    filename = f"rgwml_{args.uid}.json"
    with open(filename, 'wb') as f:
        # Resize the file to the size of the JSON output
        f.write(b' ' * len(json_output))

    with open(filename, 'r+b') as f:
        mm = mmap.mmap(f.fileno(), 0)
        mm.write(json_output.encode('utf-8'))
        mm.close()



if __name__ == '__main__':
    main()"#;

/// Python script content for dask_differentiator_connect.py
pub const DASK_DIFFERENTIATOR_CONNECT_SCRIPT: &str = r#"import argparse
import dask.dataframe as dd
import json
import mmap

def perform_difference(uid, file_a_path, file_b_path, difference_type, file_a_ref_column, file_b_ref_column):
    # Read CSV files into Dask DataFrames
    df_a = dd.read_csv(file_a_path, dtype='object', on_bad_lines='skip', low_memory=False)
    df_b = dd.read_csv(file_b_path, dtype='object', on_bad_lines='skip', low_memory=False)

    # Ensure the reference columns are present in both DataFrames
    if file_a_ref_column not in df_a.columns or file_b_ref_column not in df_b.columns:
        print(f"Error: Reference column '{file_a_ref_column}' or '{file_b_ref_column}' not found in the CSV files.")
        return

    # Perform the difference operation
    if difference_type == 'NORMAL':
        # Rows in df_a but not in df_b
        result = df_a[~df_a[file_a_ref_column].isin(df_b[file_b_ref_column])]
    elif difference_type == 'SYMMETRIC':
        # Rows in df_a but not in df_b and rows in df_b but not in df_a
        result_a_not_b = df_a[~df_a[file_a_ref_column].isin(df_b[file_b_ref_column])]
        result_b_not_a = df_b[~df_b[file_b_ref_column].isin(df_a[file_a_ref_column])]
        result = dd.concat([result_a_not_b, result_b_not_a])
    else:
        print(f"Error: Unsupported difference type '{difference_type}'.")
        return

    # Compute the result
    result = result.compute()

    # Replace NaN values with empty strings
    result = result.fillna('')

    # Prepare the final output
    headers = list(result.columns)
    rows = result.values.tolist()
    output = {
        "headers": headers,
        "rows": [[str(item) for item in row] for row in rows],
    }

    #print(json.dumps(output, indent=4))
    json_output = json.dumps(output, indent=4)
    """
    with open('output.json', 'wb') as f:
        # Resize the file to the size of the JSON output
        f.write(b' ' * len(json_output))

    with open('output.json', 'r+b') as f:
        mm = mmap.mmap(f.fileno(), 0)
        mm.write(json_output.encode('utf-8'))
        mm.close()
    """
    filename = f"rgwml_{uid}.json"
    with open(filename, 'wb') as f:
        # Resize the file to the size of the JSON output
        f.write(b' ' * len(json_output))

    with open(filename, 'r+b') as f:
        mm = mmap.mmap(f.fileno(), 0)
        mm.write(json_output.encode('utf-8'))
        mm.close()



def main():
    parser = argparse.ArgumentParser(description='Perform MySQL-like differences on CSV file datasets using Dask')

    parser.add_argument('--uid', type=str, help='A unique identifier to name the output json file', required=True)

    parser.add_argument('--file_a_path', type=str, required=True, help='Path to the first CSV file')
    parser.add_argument('--file_b_path', type=str, required=True, help='Path to the second CSV file')
    parser.add_argument('--difference_type', type=str, required=True, choices=['NORMAL', 'SYMMETRIC'], help='Type of difference operation to perform')
    parser.add_argument('--file_a_ref_column', type=str, required=True, help='Reference column in the first CSV file')
    parser.add_argument('--file_b_ref_column', type=str, required=True, help='Reference column in the second CSV file')

    args = parser.parse_args()

    perform_difference(args.uid, args.file_a_path, args.file_b_path, args.difference_type, args.file_a_ref_column, args.file_b_ref_column)

if __name__ == '__main__':
    main()"#;

/// Python script content for dask_freq_cascading_connect.py
pub const DASK_FREQ_CASCADING_CONNECT_SCRIPT: &str = r#"import argparse
import dask.dataframe as dd
import json
import mmap

def cascading_frequency_report(csv_path, columns, limit, order_by):
    # Read CSV file into a Dask DataFrame
    df = dd.read_csv(csv_path, dtype=str, blocksize=25e6)  # Adjust blocksize as necessary

    # Split columns string into a list
    columns = [col.strip() for col in columns.split(",")]

    # Convert limit to an integer if it's not None
    if limit is not None:
        limit = int(limit)

    # Generate cascading frequency report
    def generate_cascade_report(df, columns, limit, order_by):
        if not columns:
            return None

        current_col = columns[0]
        if current_col not in df.columns:
            return None

        frequency = df[current_col].value_counts().compute()
        if limit is not None:
            frequency = frequency.nlargest(limit)
        sorted_frequency = sort_frequency(frequency, order_by)

        report = {}
        for value, count in sorted_frequency.items():
            filtered_df = df[df[current_col] == value]
            if len(columns) > 1:
                sub_report = generate_cascade_report(filtered_df, columns[1:], limit, order_by)
                if len(columns) == 2:
                    report[value] = {
                        "count": str(count),
                        f"sub_distribution({columns[1]})": sub_report
                    }
                else:
                    report[value] = {
                        "count": str(count),
                        f"sub_distribution({columns[1]})": sub_report
                    }
            else:
                report[value] = {
                    "count": str(count)
                }

        return report

    def sort_frequency(frequency, order_by):
        if order_by == "ASC":
            return dict(sorted(frequency.items(), key=lambda item: item[0]))
        elif order_by == "DESC":
            return dict(sorted(frequency.items(), key=lambda item: item[0], reverse=True))
        elif order_by == "FREQ_ASC":
            return dict(sorted(frequency.items(), key=lambda item: item[1]))
        else:  # Default to "FREQ_DESC"
            return dict(sorted(frequency.items(), key=lambda item: item[1], reverse=True))

    # Generate report
    report = generate_cascade_report(df, columns, limit, order_by)

    return report

def main():
    parser = argparse.ArgumentParser(description='Generate a cascading frequency report for specified columns in a CSV file using Dask')

    parser.add_argument('--uid', type=str, help='A unique identifier to name the output json file', required=True)

    parser.add_argument('--path', type=str, required=True, help='Path to the input CSV file')
    parser.add_argument('--columns', type=str, required=True, help='Comma-separated list of columns to generate frequency report for')
    parser.add_argument('--limit', type=str, default=None, help='Limit the frequency report to the top N items')
    parser.add_argument('--order_by', type=str, choices=['ASC', 'DESC', 'FREQ_ASC', 'FREQ_DESC'], default='FREQ_DESC', help='Order of the frequency report: ASC, DESC, FREQ_ASC, FREQ_DESC')

    args = parser.parse_args()
    limit = int(args.limit) if args.limit is not None else None
    report = cascading_frequency_report(args.path, args.columns, limit, args.order_by)

    #print(json.dumps(report, indent=4))
    json_output = json.dumps(report, indent=4)
    """
    with open('output.json', 'wb') as f:
        # Resize the file to the size of the JSON output
        f.write(b' ' * len(json_output))

    with open('output.json', 'r+b') as f:
        mm = mmap.mmap(f.fileno(), 0)
        mm.write(json_output.encode('utf-8'))
        mm.close()
    """
    filename = f"rgwml_{args.uid}.json"
    with open(filename, 'wb') as f:
        # Resize the file to the size of the JSON output
        f.write(b' ' * len(json_output))

    with open(filename, 'r+b') as f:
        mm = mmap.mmap(f.fileno(), 0)
        mm.write(json_output.encode('utf-8'))
        mm.close()



if __name__ == '__main__':
    main()"#;

/// Python script content for dask_freq_linear_connect.py
pub const DASK_FREQ_LINEAR_CONNECT_SCRIPT: &str = r#"import argparse
import dask.dataframe as dd
import json
import mmap

def frequency_report(csv_path, columns, limit, order_by):
    # Read CSV file into a Dask DataFrame
    df = dd.read_csv(csv_path, dtype=str, blocksize=25e6)  # Adjust blocksize as necessary

    # Split columns string into a list
    columns = [col.strip() for col in columns.split(",")]

    # Generate frequency report
    frequency_report = {"unique_value_counts": {}, "column_analysis": {}}
    for col in columns:
        if col in df.columns:
            frequency = df[col].value_counts().compute()
            unique_count = frequency.size

            if limit is not None:
                top_frequency = frequency.nlargest(limit).to_dict()
            else:
                top_frequency = frequency.to_dict()

            # Sort the frequency dictionary based on the order_by flag
            if order_by == "ASC":
                sorted_frequency = dict(sorted(top_frequency.items(), key=lambda item: item[0]))
            elif order_by == "DESC":
                sorted_frequency = dict(sorted(top_frequency.items(), key=lambda item: item[0], reverse=True))
            elif order_by == "FREQ_ASC":
                sorted_frequency = dict(sorted(top_frequency.items(), key=lambda item: item[1]))
            else:  # Default to "FREQ_DESC"
                sorted_frequency = dict(sorted(top_frequency.items(), key=lambda item: item[1], reverse=True))

            # Convert frequency values to strings
            sorted_frequency = {k: str(v) for k, v in sorted_frequency.items()}

            frequency_report["unique_value_counts"][col] = str(unique_count)
            frequency_report["column_analysis"][col] = sorted_frequency

    return frequency_report

def main():
    parser = argparse.ArgumentParser(description='Generate a frequency report for specified columns in a CSV file using Dask')

    parser.add_argument('--uid', type=str, help='A unique identifier to name the output json file', required=True)
    parser.add_argument('--path', type=str, required=True, help='Path to the input CSV file')
    parser.add_argument('--columns', type=str, required=True, help='Comma-separated list of columns to generate frequency report for')
    parser.add_argument('--limit', type=int, default=None, help='Limit the frequency report to the top N items')
    parser.add_argument('--order_by', type=str, choices=['ASC', 'DESC', 'FREQ_ASC', 'FREQ_DESC'], default='FREQ_DESC', help='Order of the frequency report: ASC, DESC, FREQ_ASC, FREQ_DESC')

    args = parser.parse_args()
    report = frequency_report(args.path, args.columns, args.limit, args.order_by)

    #print(json.dumps(report, indent=4))
    json_output = json.dumps(report, indent=4)
    """
    with open('output.json', 'wb') as f:
        # Resize the file to the size of the JSON output
        f.write(b' ' * len(json_output))

    with open('output.json', 'r+b') as f:
        mm = mmap.mmap(f.fileno(), 0)
        mm.write(json_output.encode('utf-8'))
        mm.close()
    """
    filename = f"rgwml_{args.uid}.json"
    with open(filename, 'wb') as f:
        # Resize the file to the size of the JSON output
        f.write(b' ' * len(json_output))

    with open(filename, 'r+b') as f:
        mm = mmap.mmap(f.fileno(), 0)
        mm.write(json_output.encode('utf-8'))
        mm.close()



if __name__ == '__main__':
    main()"#;

/// Python script content for dask_grouper_connect.py
pub const DASK_GROUPER_CONNECT_SCRIPT: &str = r#"import argparse
import dask.dataframe as dd
import json
import mmap

def process_with_dask(uid, csv_path, groupby_column, operations, limit):
    # Read CSV file into a Dask DataFrame with all columns as objects
    df = dd.read_csv(csv_path, dtype='object', on_bad_lines='skip', low_memory=False)

    if limit:
        df = df.head(int(limit), compute=False)

    if groupby_column not in df.columns:
        print(f"Error: Column '{groupby_column}' not found in the CSV file.")
        return

    # Convert appropriate columns to numerical types for aggregation
    def safe_convert(df, columns, dtype):
        for col in columns:
            if col in df.columns:
                try:
                    df[col] = dd.to_numeric(df[col], errors='coerce')
                    if dtype == 'int64':
                        df[col] = df[col].fillna(0).astype('int64')
                    else:
                        df[col] = df[col].astype(dtype)
                except ValueError:
                    print(f"Warning: Could not convert column '{col}' to {dtype}. Skipping this column.")
                    df = df.drop(columns=[col])
        return df

    numerical_columns = set()
    datetime_columns = set()
    non_numerical_columns = set()

    for op, columns in operations.items():
        if 'numerical' in op or 'bool_percent' in op:
            numerical_columns.update(columns)
        elif op in ['datetime_max', 'datetime_min']:
            datetime_columns.update(columns)
        elif op in ['datetime_semi_colon_separated', 'mode', 'count_unique']:
            non_numerical_columns.update(columns)

    # Convert numerical and datetime columns only
    df = safe_convert(df, numerical_columns, 'float64')
    for col in datetime_columns:
        if col in df.columns:
            df[col] = dd.to_datetime(df[col], errors='coerce')

    # Split operations into separate DataFrames
    grouped_dfs = []

    def compute_and_rename(df, agg_dict, rename_dict):
        grouped_df = df.groupby(groupby_column).agg(agg_dict).reset_index().compute()
        grouped_df = grouped_df.rename(columns=rename_dict)
        return grouped_df

    for op, columns in operations.items():
        for col in columns:
            if col in df.columns:
                if op == 'numerical_max':
                    agg_dict = {col: 'max'}
                    rename_dict = {col: f'{col}_NUMERICAL_MAX'}
                elif op == 'numerical_min':
                    agg_dict = {col: 'min'}
                    rename_dict = {col: f'{col}_NUMERICAL_MIN'}
                elif op == 'numerical_sum':
                    agg_dict = {col: 'sum'}
                    rename_dict = {col: f'{col}_NUMERICAL_SUM'}
                elif op == 'numerical_mean':
                    agg_dict = {col: 'mean'}
                    rename_dict = {col: f'{col}_NUMERICAL_MEAN'}
                elif op == 'numerical_median':
                    agg_dict = {col: 'median'}
                    rename_dict = {col: f'{col}_NUMERICAL_MEDIAN'}
                elif op == 'numerical_std':
                    agg_dict = {col: 'std'}
                    rename_dict = {col: f'{col}_NUMERICAL_STANDARD_DEVIATION'}
                elif op == 'datetime_max':
                    agg_dict = {col: 'max'}
                    rename_dict = {col: f'{col}_DATETIME_MAX'}
                elif op == 'datetime_min':
                    agg_dict = {col: 'min'}
                    rename_dict = {col: f'{col}_DATETIME_MIN'}
                elif op == 'datetime_semi_colon_separated':
                    grouped_df = df.groupby(groupby_column)[col].apply(
                        lambda x: ';'.join(x.dropna().astype(str).replace('NaT', '')), meta=(col, 'str')
                    ).reset_index().compute()
                    grouped_df = grouped_df.rename(columns={col: f'{col}_DATETIME_SEMI_COLON_SEPARATED'})
                    grouped_dfs.append(grouped_df)
                    continue
                elif op == 'bool_percent':
                    # Ensure the column is numeric and drop non-numeric values
                    df[col] = dd.to_numeric(df[col], errors='coerce')
                    df = df.dropna(subset=[col])
                    grouped_df = df.groupby(groupby_column)[col].apply(lambda x: round((x.sum() / x.count()) * 100, 2), meta=(col, 'float64')).reset_index().compute()
                    grouped_df = grouped_df.rename(columns={col: f'{col}_BOOL_PERCENT'})
                    grouped_dfs.append(grouped_df)
                    continue
                elif op == 'mode':
                    grouped_df = df.groupby(groupby_column)[col].apply(lambda x: x.mode().iloc[0] if not x.mode().empty else None, meta=(col, 'object')).reset_index().compute()
                    grouped_df = grouped_df.rename(columns={col: f'{col}_MODE'})
                    grouped_dfs.append(grouped_df)
                    continue
                else:
                    continue

                grouped_df = compute_and_rename(df, agg_dict, rename_dict)
                grouped_dfs.append(grouped_df)

    # Handle count_unique operation separately
    count_unique_cols = operations.get('count_unique', [])
    for col in count_unique_cols:
        if col in df.columns:
            count_unique_df = df.groupby(groupby_column)[col].nunique().reset_index().compute()
            count_unique_df = count_unique_df.rename(columns={col: f'{col}_COUNT_UNIQUE'})
            grouped_dfs.append(count_unique_df)

    # Compute COUNT_TOTAL for the groupby column values
    count_total_df = df.groupby(groupby_column).size().to_frame('COUNT_TOTAL').reset_index().compute()
    grouped_dfs.append(count_total_df)

    # Compute COUNT_UNIQUE for the groupby column values
    """
    count_unique_groupby_df = df.groupby(groupby_column).apply(lambda x: x[groupby_column].nunique(), meta=(groupby_column, 'int64')).to_frame('COUNT_UNIQUE').reset_index().compute()
    grouped_dfs.append(count_unique_groupby_df)
    """
    count_unique_groupby_df = df.drop_duplicates().groupby(groupby_column).size().to_frame('COUNT_UNIQUE').reset_index().compute()
    grouped_dfs.append(count_unique_groupby_df)

    # Join all the DataFrames together
    final_grouped_df = grouped_dfs[0]
    for grouped_df in grouped_dfs[1:]:
        final_grouped_df = final_grouped_df.merge(grouped_df, on=groupby_column, how='outer')

    # Round specific columns to 2 decimal places
    for col in final_grouped_df.columns:
        if col.endswith('_NUMERICAL_SUM') or col.endswith('_NUMERICAL_MEAN') or col.endswith('_NUMERICAL_MEDIAN') or col.endswith('_NUMERICAL_STANDARD_DEVIATION'):
            final_grouped_df[col] = final_grouped_df[col].round(2)

    # Reorder columns to ensure COUNT_TOTAL and COUNT_UNIQUE follow the groupby column, and rest in alphabetical order
    cols = [groupby_column, 'COUNT_TOTAL', 'COUNT_UNIQUE'] + sorted([col for col in final_grouped_df.columns if col not in {groupby_column, 'COUNT_TOTAL', 'COUNT_UNIQUE'}])
    final_grouped_df = final_grouped_df[cols]

    # Convert all columns to string to ensure consistency
    final_grouped_df = final_grouped_df.astype(str)

    # Replace 'NaT' with empty string in all columns
    final_grouped_df.replace('NaT', '', inplace=True)

    # Prepare the final output
    headers = list(final_grouped_df.columns)
    rows = final_grouped_df.values.tolist()
    output = {
        "headers": headers,
        "rows": [[item for item in row] for row in rows],
    }

    #print(json.dumps(output, indent=4))
    json_output = json.dumps(output, indent=4)

    """
    with open('output.json', 'wb') as f:
        # Resize the file to the size of the JSON output
        f.write(b' ' * len(json_output))

    with open('output.json', 'r+b') as f:
        mm = mmap.mmap(f.fileno(), 0)
        mm.write(json_output.encode('utf-8'))
        mm.close()
    """
    filename = f"rgwml_{uid}.json"
    with open(filename, 'wb') as f:
        # Resize the file to the size of the JSON output
        f.write(b' ' * len(json_output))

    with open(filename, 'r+b') as f:
        mm = mmap.mmap(f.fileno(), 0)
        mm.write(json_output.encode('utf-8'))
        mm.close()



def parse_columns(columns_str):
    return [col.strip() for col in columns_str.split(',')]

def main():
    parser = argparse.ArgumentParser(description='Process a CSV file using Dask')
    parser.add_argument('--uid', type=str, help='A unique identifier to name the output json file', required=True)

    parser.add_argument('--path', type=str, required=True, help='Path to the input CSV file')
    parser.add_argument('--group_by', type=str, required=True, help='Column name to group by')
    parser.add_argument('--numerical_max', type=str, help='Columns to find max values, separated by commas')
    parser.add_argument('--numerical_min', type=str, help='Columns to find min values, separated by commas')
    parser.add_argument('--numerical_sum', type=str, help='Columns to sum values, separated by commas')
    parser.add_argument('--numerical_mean', type=str, help='Columns to find mean values, separated by commas')
    parser.add_argument('--numerical_median', type=str, help='Columns to find median values, separated by commas')
    parser.add_argument('--numerical_std', type=str, help='Columns to find standard deviation, separated by commas')
    parser.add_argument('--datetime_max', type=str, help='Columns to find max datetime values, separated by commas')
    parser.add_argument('--datetime_min', type=str, help='Columns to find min datetime values, separated by commas')
    parser.add_argument('--datetime_semi_colon_separated', type=str, help='Columns to semi-colon separate datetime values, separated by commas')
    parser.add_argument('--mode', type=str, help='Columns to find the most frequent value, separated by commas')
    parser.add_argument('--bool_percent', type=str, help='Columns to calculate the percentage of 1s, separated by commas')
    parser.add_argument('--count_unique', type=str, help='Columns to count unique values, separated by commas')
    parser.add_argument('--limit', type=str, help='Limit the number of rows to process')

    args = parser.parse_args()

    operations = {
        'numerical_max': parse_columns(args.numerical_max) if args.numerical_max else [],
        'numerical_min': parse_columns(args.numerical_min) if args.numerical_min else [],
        'numerical_sum': parse_columns(args.numerical_sum) if args.numerical_sum else [],
        'numerical_mean': parse_columns(args.numerical_mean) if args.numerical_mean else [],
        'numerical_median': parse_columns(args.numerical_median) if args.numerical_median else [],
        'numerical_std': parse_columns(args.numerical_std) if args.numerical_std else [],
        'datetime_max': parse_columns(args.datetime_max) if args.datetime_max else [],
        'datetime_min': parse_columns(args.datetime_min) if args.datetime_min else [],
        'datetime_semi_colon_separated': parse_columns(args.datetime_semi_colon_separated) if args.datetime_semi_colon_separated else [],
        'mode': parse_columns(args.mode) if args.mode else [],
        'bool_percent': parse_columns(args.bool_percent) if args.bool_percent else [],
        'count_unique': parse_columns(args.count_unique) if args.count_unique else [],
    }

    process_with_dask(args.uid, args.path, args.group_by, operations, args.limit)

if __name__ == '__main__':
    main()"#;

/// Python script content for dask_inspect_connect.py
pub const DASK_INSPECT_CONNECT_SCRIPT: &str = r#"import argparse
import dask.dataframe as dd
import pandas as pd
import json
import numpy as np
import mmap

def retrieve_rows(df, method):
    if method == "GET_FIRST_ROW":
        subset = df.head(1)
    elif method == "GET_LAST_ROW":
        subset = df.tail(1)
    elif method.startswith("GET_FIRST_N_ROWS:"):
        n = int(method.split(":")[1])
        subset = df.head(n)
    elif method.startswith("GET_LAST_N_ROWS:"):
        n = int(method.split(":")[1])
        subset = df.tail(n)
    elif method.startswith("GET_ROW_RANGE:"):
        start, end = map(int, method.split(":")[1].split("_"))
        subset = df.loc[start:end+1]
    elif method == "GET_SUMMARY":
        first_five = df.head(5)
        last_five = df.tail(5)
        subset = dd.concat([first_five, last_five])
    else:
        raise ValueError("Invalid method")

    if isinstance(subset, dd.DataFrame):
        subset = subset.compute()

    # Convert all NaN values to empty strings
    subset = subset.fillna("")

    # Extract row numbers and adjust to start from 1
    row_numbers = (subset.index + 1).tolist()
    records = subset.astype(str).to_dict(orient="records")

    # Create a dictionary with row numbers as keys
    result = {str(row_number): record for row_number, record in zip(row_numbers, records)}

    return result

def retrieve_row_range_in_chunks(path, start, end):
    result = {}
    start = start - 1
    end = end - 1

    # Read specified rows using pandas directly
    df_chunk = pd.read_csv(path, skiprows=range(1, start + 1), nrows=end - start + 1, dtype=str)

    # Convert all NaN values to empty strings
    df_chunk = df_chunk.fillna("")

    # Adjust index to match original row numbers
    df_chunk.index += start

    row_numbers = (df_chunk.index + 1).tolist()
    records = df_chunk.astype(str).to_dict(orient="records")

    # Update the result dictionary
    result.update({str(row_number): record for row_number, record in zip(row_numbers, records)})

    return result

def main():
    parser = argparse.ArgumentParser(description='Retrieve specific rows from a CSV file using Dask')
    parser.add_argument('--uid', type=str, help='A unique identifier to name the output json file', required=True)
    parser.add_argument('--path', type=str, required=True, help='Path to the input CSV file')
    parser.add_argument('--method', type=str, required=True, help='Method to retrieve rows: GET_FIRST_ROW, GET_LAST_ROW, GET_FIRST_N_ROWS:X, GET_LAST_N_ROWS:Y, GET_ROW_RANGE:Z_A, GET_SUMMARY')
    parser.add_argument('--blocksize', type=int, default=78643200, help='Block size in bytes for chunk processing (default: 75MB)')
    parser.add_argument('--sample_size', type=int, default=10000000, help='Sample size in bytes for reading CSV (default: 10MB)')

    args = parser.parse_args()

    # Read CSV file into a Dask DataFrame
    df = dd.read_csv(args.path, dtype=str, blocksize=args.blocksize, sample=args.sample_size)

    # Retrieve rows based on the method specified
    if args.method.startswith("GET_ROW_RANGE:"):
        start, end = map(int, args.method.split(":")[1].split("_"))
        selected_rows = retrieve_row_range_in_chunks(args.path, start, end)
    else:
        selected_rows = retrieve_rows(df, args.method)

    # Convert the dictionary to a JSON string
    #selected_rows_json = json.dumps({"rows": selected_rows}, indent=4)

    #print(selected_rows_json)
    json_output = json.dumps({"rows": selected_rows}, indent=4)
    """
    with open('output.json', 'wb') as f:
        # Resize the file to the size of the JSON output
        f.write(b' ' * len(json_output))

    with open('output.json', 'r+b') as f:
        mm = mmap.mmap(f.fileno(), 0)
        mm.write(json_output.encode('utf-8'))
        mm.close()
    """
    filename = f"rgwml_{args.uid}.json"
    with open(filename, 'wb') as f:
        # Resize the file to the size of the JSON output
        f.write(b' ' * len(json_output))

    with open(filename, 'r+b') as f:
        mm = mmap.mmap(f.fileno(), 0)
        mm.write(json_output.encode('utf-8'))
        mm.close()



if __name__ == '__main__':
    main()"#;

/// Python script content for dask_intersector_connect.py
pub const DASK_INTERSECTOR_CONNECT_SCRIPT: &str = r#"import argparse
import dask.dataframe as dd
import json
import mmap

def perform_intersection(uid, file_a_path, file_b_path, file_a_ref_column, file_b_ref_column):
    # Read CSV files into Dask DataFrames
    df_a = dd.read_csv(file_a_path, dtype='object', on_bad_lines='skip', low_memory=False)
    df_b = dd.read_csv(file_b_path, dtype='object', on_bad_lines='skip', low_memory=False)

    # Ensure the reference columns are present in both DataFrames
    if file_a_ref_column not in df_a.columns or file_b_ref_column not in df_b.columns:
        print(f"Error: Reference column '{file_a_ref_column}' or '{file_b_ref_column}' not found in the CSV files.")
        return

    # Perform the intersection operation (inner join)
    result = dd.merge(df_a, df_b, left_on=file_a_ref_column, right_on=file_b_ref_column, how='inner')

    # Compute the result
    result = result.compute()

    # Replace NaN values with empty strings
    result = result.fillna('')

    # Prepare the final output
    headers = list(result.columns)
    rows = result.values.tolist()
    output = {
        "headers": headers,
        "rows": [[str(item) for item in row] for row in rows],
    }

    
    #print(json.dumps(output, indent=4))
    json_output = json.dumps(output, indent=4)
    """
    with open('output.json', 'wb') as f:
        # Resize the file to the size of the JSON output
        f.write(b' ' * len(json_output))

    with open('output.json', 'r+b') as f:
        mm = mmap.mmap(f.fileno(), 0)
        mm.write(json_output.encode('utf-8'))
        mm.close()
    """
    filename = f"rgwml_{uid}.json"
    with open(filename, 'wb') as f:
        # Resize the file to the size of the JSON output
        f.write(b' ' * len(json_output))

    with open(filename, 'r+b') as f:
        mm = mmap.mmap(f.fileno(), 0)
        mm.write(json_output.encode('utf-8'))
        mm.close()



def main():
    parser = argparse.ArgumentParser(description='Perform MySQL-like intersections on CSV file datasets using Dask')
    parser.add_argument('--uid', type=str, help='A unique identifier to name the output json file', required=True)
    parser.add_argument('--file_a_path', type=str, required=True, help='Path to the first CSV file')
    parser.add_argument('--file_b_path', type=str, required=True, help='Path to the second CSV file')
    parser.add_argument('--file_a_ref_column', type=str, required=True, help='Reference column in the first CSV file')
    parser.add_argument('--file_b_ref_column', type=str, required=True, help='Reference column in the second CSV file')

    args = parser.parse_args()

    perform_intersection(args.uid, args.file_a_path, args.file_b_path, args.file_a_ref_column, args.file_b_ref_column)

if __name__ == '__main__':
    main()"#;

/// Python script content for dask_joiner_connect.py
pub const DASK_JOINER_CONNECT_SCRIPT: &str = r#"import argparse
import dask.dataframe as dd
import json
import mmap

def perform_join(uid, file_a_path, file_b_path, join_type, file_a_ref_column, file_b_ref_column):
    # Read CSV files into Dask DataFrames
    df_a = dd.read_csv(file_a_path, dtype='object', on_bad_lines='skip', low_memory=False)
    df_b = dd.read_csv(file_b_path, dtype='object', on_bad_lines='skip', low_memory=False)

    # Ensure the reference columns are present in both DataFrames
    if join_type not in ['UNION', 'BAG_UNION'] and (file_a_ref_column not in df_a.columns or file_b_ref_column not in df_b.columns):
        print(f"Error: Reference column '{file_a_ref_column}' or '{file_b_ref_column}' not found in the CSV files.")
        return

    # Perform the join operation
    if join_type == 'LEFT_JOIN':
        result = dd.merge(df_a, df_b, left_on=file_a_ref_column, right_on=file_b_ref_column, how='left')
    elif join_type == 'RIGHT_JOIN':
        result = dd.merge(df_a, df_b, left_on=file_a_ref_column, right_on=file_b_ref_column, how='right')
    elif join_type == 'OUTER_FULL_JOIN':
        result = dd.merge(df_a, df_b, left_on=file_a_ref_column, right_on=file_b_ref_column, how='outer')
    elif join_type == 'UNION':
        result = dd.concat([df_a, df_b]).drop_duplicates().reset_index(drop=True)
    elif join_type == 'BAG_UNION':
        result = dd.concat([df_a, df_b]).reset_index(drop=True)
    else:
        print(f"Error: Unsupported join type '{join_type}'.")
        return

    # Compute the result
    result = result.compute()

    # Replace NaN values with empty strings
    result = result.fillna('')

    # Prepare the final output
    headers = list(result.columns)
    rows = result.values.tolist()
    output = {
        "headers": headers,
        "rows": [[str(item) for item in row] for row in rows],
    }

    json_output = json.dumps(output, indent=4)
    """
    with open('output.json', 'wb') as f:
        # Resize the file to the size of the JSON output
        f.write(b' ' * len(json_output))

    with open('output.json', 'r+b') as f:
        mm = mmap.mmap(f.fileno(), 0)
        mm.write(json_output.encode('utf-8'))
        mm.close()
    """
    filename = f"rgwml_{uid}.json"
    with open(filename, 'wb') as f:
        # Resize the file to the size of the JSON output
        f.write(b' ' * len(json_output))

    with open(filename, 'r+b') as f:
        mm = mmap.mmap(f.fileno(), 0)
        mm.write(json_output.encode('utf-8'))
        mm.close()



def main():
    parser = argparse.ArgumentParser(description='Perform MySQL-like joins on CSV file datasets using Dask')
    
    parser.add_argument('--uid', type=str, help='A unique identifier to name the output json file', required=True)

    parser.add_argument('--file_a_path', type=str, required=True, help='Path to the first CSV file')
    parser.add_argument('--file_b_path', type=str, required=True, help='Path to the second CSV file')
    parser.add_argument('--join_type', type=str, required=True, choices=['LEFT_JOIN', 'RIGHT_JOIN', 'OUTER_FULL_JOIN', 'UNION', 'BAG_UNION'], help='Type of join operation to perform')
    parser.add_argument('--file_a_ref_column', type=str, required=False, help='Reference column in the first CSV file')
    parser.add_argument('--file_b_ref_column', type=str, required=False, help='Reference column in the second CSV file')

    args = parser.parse_args()

    perform_join(args.uid, args.file_a_path, args.file_b_path, args.join_type, args.file_a_ref_column, args.file_b_ref_column)

if __name__ == '__main__':
    main()"#;

/// Python script content for dask_pivoter_connect.py
pub const DASK_PIVOTER_CONNECT_SCRIPT: &str = r#"import argparse
import dask.dataframe as dd
import pandas as pd
import json
import mmap

def process_with_dask(uid, csv_path, groupby_column, values_from, operation, segregate_by, limit):
    # Read CSV file into a Dask DataFrame with all columns as objects
    df = dd.read_csv(csv_path, dtype='object', on_bad_lines='skip', low_memory=False)

    if limit:
        df = df.head(int(limit), compute=False)

    if groupby_column not in df.columns or values_from not in df.columns:
        print(f"Error: Column '{groupby_column}' or '{values_from}' not found in the CSV file.")
        return

    for col in segregate_by:
        if col not in df.columns:
            print(f"Error: Column '{col}' not found in the CSV file.")
            return

    # Apply segregations, treating all as categories
    for col in segregate_by:
        if col in df.columns:
            df[col] = df[col].astype('category')

    # Convert appropriate columns to numerical types for aggregation
    def safe_convert(df, col, dtype):
        if col in df.columns:
            try:
                df[col] = dd.to_numeric(df[col], errors='coerce')
                if dtype == 'int64':
                    df[col] = df[col].fillna(0).astype('int64')
                else:
                    df[col] = df[col].astype(dtype)
            except ValueError:
                print(f"Warning: Could not convert column '{col}' to {dtype}. Skipping this column.")
                df = df.drop(columns=[col])
        return df

    if operation in ['NUMERICAL_MAX', 'NUMERICAL_MIN', 'NUMERICAL_SUM', 'NUMERICAL_MEAN', 'NUMERICAL_MEDIAN', 'NUMERICAL_STANDARD_DEVIATION']:
        df = safe_convert(df, values_from, 'float64')

    if operation == 'BOOL_PERCENT':
        df = safe_convert(df, values_from, 'float64')

    if operation in ['DATETIME_MAX', 'DATETIME_MIN']:
        df[values_from] = dd.to_datetime(df[values_from], errors='coerce')

    # Define aggregation operations
    aggfunc = None
    if operation == 'COUNT':
        aggfunc = 'count'
    elif operation == 'COUNT_UNIQUE':
        aggfunc = 'nunique'
    elif operation == 'NUMERICAL_MAX':
        aggfunc = 'max'
    elif operation == 'NUMERICAL_MIN':
        aggfunc = 'min'
    elif operation == 'NUMERICAL_SUM':
        aggfunc = 'sum'
    elif operation == 'NUMERICAL_MEAN':
        aggfunc = 'mean'
    elif operation == 'NUMERICAL_MEDIAN':
        aggfunc = 'median'
    elif operation == 'NUMERICAL_STANDARD_DEVIATION':
        aggfunc = 'std'
    elif operation == 'BOOL_PERCENT':
        aggfunc = lambda x: round((x.sum() / x.count()) * 100, 2)
    else:
        print(f"Error: Unsupported operation '{operation}'.")
        return

    # Ensure columns used for segregation are properly set before pivoting
    for col in segregate_by:
        if df[col].dtype.name == 'category':
            df[col] = df[col].astype(str)

    # Create pivot table
    if segregate_by:
        pivot_df = df.compute().pivot_table(index=groupby_column, columns=list(segregate_by), values=values_from, aggfunc=aggfunc)

        # Fill NaN values with 0
        pivot_df = pivot_df.fillna(0)

        # Round values to 2 decimal places
        pivot_df = pivot_df.round(2)

        # Create column names in the desired format
        new_columns = []
        for col in pivot_df.columns.to_flat_index():
            col_name = "&&".join([f"[{key}({val})]" for key, val in zip(segregate_by, col)])
            new_columns.append(col_name)

        # Set new column names
        pivot_df.columns = new_columns

        # Add TOTAL column
        pivot_df[f'OVERALL_{operation}({values_from})'] = pivot_df.sum(axis=1).round(2)

        # Reset index and ensure groupby_column is the first column
        pivot_df = pivot_df.reset_index()

    else:
        pivot_df = df.compute().groupby(groupby_column).agg({values_from: aggfunc}).reset_index()
        pivot_df.columns = [groupby_column, f'{operation}({values_from})']

        # Fill NaN values with 0
        pivot_df = pivot_df.fillna(0)

        # Round values to 2 decimal places
        pivot_df = pivot_df.round(2)

    # Prepare the final output
    headers = list(pivot_df.columns)
    rows = pivot_df.values.tolist()
    output = {
        "headers": headers,
        "rows": [[str(item) for item in row] for row in rows],
    }

    #print(json.dumps(output, indent=4))
    json_output = json.dumps(output, indent=4)
    """
    with open('output.json', 'wb') as f:
        # Resize the file to the size of the JSON output
        f.write(b' ' * len(json_output))

    with open('output.json', 'r+b') as f:
        mm = mmap.mmap(f.fileno(), 0)
        mm.write(json_output.encode('utf-8'))
        mm.close()
    """
    filename = f"rgwml_{uid}.json"
    with open(filename, 'wb') as f:
        # Resize the file to the size of the JSON output
        f.write(b' ' * len(json_output))

    with open(filename, 'r+b') as f:
        mm = mmap.mmap(f.fileno(), 0)
        mm.write(json_output.encode('utf-8'))
        mm.close()



def parse_columns(columns_str):
    return [col.strip() for col in columns_str.split(',')]

def parse_segregate_by(segregate_by_str):
    return [col.strip() for col in segregate_by_str.split(',')]

def main():
    parser = argparse.ArgumentParser(description='Process a CSV file using Dask and create a pivot table')

    parser.add_argument('--uid', type=str, help='A unique identifier to name the output json file', required=True)

    parser.add_argument('--path', type=str, required=True, help='Path to the input CSV file')
    parser.add_argument('--group_by', type=str, required=True, help='Column name to group by')
    parser.add_argument('--values_from', type=str, required=True, help='Column name to take values from for aggregation')
    parser.add_argument('--operation', type=str, required=True, choices=[
        'COUNT', 'COUNT_UNIQUE', 'NUMERICAL_MAX', 'NUMERICAL_MIN', 'NUMERICAL_SUM',
        'NUMERICAL_MEAN', 'NUMERICAL_MEDIAN', 'NUMERICAL_STANDARD_DEVIATION', 'BOOL_PERCENT'
    ], help='Operation to perform for aggregation')
    parser.add_argument('--segregate_by', type=str, help='Column names for segregation, separated by commas')
    parser.add_argument('--limit', type=str, help='Limit the number of rows to process')

    args = parser.parse_args()

    segregate_by = parse_segregate_by(args.segregate_by) if args.segregate_by else []

    process_with_dask(args.uid, args.path, args.group_by, args.values_from, args.operation, segregate_by, args.limit)

if __name__ == '__main__':
    main()"#;

/// Python script content for dask_unique_value_stats_connect.py
pub const DASK_UNIQUE_VALUE_STATS_CONNECT_SCRIPT: &str = r#"import argparse
import dask.dataframe as dd
import json
import mmap

def calculate_unique_value_stats(df, columns):
    stats = {}
    for col in columns:
        if col not in df.columns:
            print(f"Column '{col}' not found in the CSV file.")
            continue

        # Calculate frequency of each unique value
        freq_map = df[col].value_counts().compute().to_dict()

        # Calculate statistics
        total_unique_values = len(freq_map)
        frequencies = list(freq_map.values())
        total_frequency = sum(frequencies)
        mean_frequency = total_frequency / total_unique_values if total_unique_values > 0 else 0

        # Calculate the median frequency
        sorted_frequencies = sorted(frequencies)
        mid = len(sorted_frequencies) // 2
        if len(sorted_frequencies) % 2 == 0:
            median_frequency = (sorted_frequencies[mid - 1] + sorted_frequencies[mid]) / 2.0
        else:
            median_frequency = sorted_frequencies[mid]

        # Store the statistics
        stats[col] = {
            "total_unique_values": total_unique_values,
            "mean_frequency": mean_frequency,
            "median_frequency": median_frequency
        }

    return stats

def process_csv_with_dask(csv_path, columns):
    # Read CSV file into a Dask DataFrame
    df = dd.read_csv(csv_path, dtype='object')

    # Calculate unique value statistics for specified columns
    output = calculate_unique_value_stats(df, columns)

    # Convert the statistics to JSON format and store in a single variable
    #json_output = json.dumps(stats, indent=4)
    
    # Print the JSON string
    #print(json_output)
    
    # Store the JSON string in a variable
    #json_result = json_output
    return output

def parse_columns(columns_str):
    return [col.strip() for col in columns_str.split(',')]

def main():
    parser = argparse.ArgumentParser(description='Calculate unique value statistics for specified columns in a CSV file using Dask')

    parser.add_argument('--uid', type=str, help='A unique identifier to name the output json file', required=True)

    parser.add_argument('--path', type=str, required=True, help='Path to the input CSV file')
    parser.add_argument('--columns', type=str, required=True, help='Comma-separated list of column names to calculate statistics for')

    args = parser.parse_args()
    columns = parse_columns(args.columns)

    output = process_csv_with_dask(args.path, columns)
    # You can now use json_result as needed in your script
    json_output = json.dumps(output, indent=4)
    """
    with open('output.json', 'wb') as f:
        # Resize the file to the size of the JSON output
        f.write(b' ' * len(json_output))

    with open('output.json', 'r+b') as f:
        mm = mmap.mmap(f.fileno(), 0)
        mm.write(json_output.encode('utf-8'))
        mm.close()
    """
    filename = f"rgwml_{args.uid}.json"
    with open(filename, 'wb') as f:
        # Resize the file to the size of the JSON output
        f.write(b' ' * len(json_output))

    with open(filename, 'r+b') as f:
        mm = mmap.mmap(f.fileno(), 0)
        mm.write(json_output.encode('utf-8'))
        mm.close()



if __name__ == '__main__':
    main()"#;

/// Python script content for db_connect.py
pub const DB_CONNECT_SCRIPT: &str = r#"from clickhouse_driver import Client
from google.cloud import bigquery
import sys
import json
import mmap

def create_clickhouse_connection(host, port, username, password):
    client = Client(host=host, port=port, user=username, password=password)
    return client

def query_clickhouse(client, query):
    results, columns = client.execute(query, with_column_types=True)
    return results, columns

def create_bigquery_connection(credentials_path):
    client = bigquery.Client.from_service_account_json(credentials_path)
    return client

def query_bigquery(client, query):
    query_job = client.query(query)
    results = query_job.result()
    columns = [field.name for field in results.schema]
    rows = [list(row.values()) for row in results]
    return rows, columns

def main():
    if len(sys.argv) < 3:
        print("Usage: python script.py <uid> <database_type> <database_specific_arguments>")
        sys.exit(1)

    uid = sys.argv[1]
    database_type = sys.argv[2]

    if database_type == "clickhouse":
        if len(sys.argv) != 8:
            print("Usage for ClickHouse: python script.py clickhouse <host> <port> <username> <password> <query>")
            sys.exit(1)

        host = sys.argv[3]
        port = sys.argv[4]
        username = sys.argv[5]
        password = sys.argv[6]
        query = sys.argv[7]

        client = create_clickhouse_connection(host, port, username, password)
        results, columns = query_clickhouse(client, query)

    elif database_type == "google_big_query":
        if len(sys.argv) != 5:
            print("Usage for Google BigQuery: python script.py google_big_query <credentials_path> <query>")
            sys.exit(1)

        credentials_path = sys.argv[3]
        query = sys.argv[4]

        client = create_bigquery_connection(credentials_path)
        results, columns = query_bigquery(client, query)

    else:
        print("Unsupported database type. Use 'clickhouse' or 'google_big_query'.")
        sys.exit(1)

    output = {
        "headers": columns,
        "rows": [[str(item) for item in row] for row in results]
    }

    #print(json.dumps(output))
    json_output = json.dumps(output, indent=4)
    with open('output.json', 'wb') as f:
        # Resize the file to the size of the JSON output
        f.write(b' ' * len(json_output))

    with open('output.json', 'r+b') as f:
        mm = mmap.mmap(f.fileno(), 0)
        mm.write(json_output.encode('utf-8'))
        mm.close()

if __name__ == "__main__":
    main()"#;

/// Python script content for dc_connect.py
pub const DC_CONNECT_SCRIPT: &str = r#"import argparse
import json
import h5py
import pandas as pd
import mmap

def read_h5_data(file_path, dataset_identifier, identifier_type):
    with h5py.File(file_path, 'r') as f:
        if identifier_type == "DATASET_NAME":
            if dataset_identifier in f:
                dataset = f[dataset_identifier][:]
            else:
                raise ValueError(f"Dataset {dataset_identifier} not found in the file.")
        elif identifier_type == "DATASET_ID":
            dataset_names = list(f.keys())
            try:
                index = int(dataset_identifier)
                if 0 <= index < len(dataset_names):
                    dataset_name = dataset_names[index]
                    dataset = f[dataset_name][:]
                else:
                    raise ValueError(f"Dataset index {index} out of range.")
            except ValueError:
                raise ValueError("Invalid dataset ID.")
        else:
            raise ValueError("Invalid identifier type.")

        headers = [f"Column {i}" for i in range(dataset.shape[1])] if dataset.ndim > 1 else ["Value"]
        rows = dataset.tolist()

    return headers, rows

def read_pandas_h5(file_path, dataset_identifier, identifier_type):
    with h5py.File(file_path, 'r') as f:
        dataset_names = list(f.keys())
        if identifier_type == "DATASET_ID":
            try:
                index = int(dataset_identifier)
                if 0 <= index < len(dataset_names):
                    key = dataset_names[index]
                else:
                    raise ValueError(f"Dataset index {index} out of range.")
            except ValueError:
                raise ValueError("Invalid dataset ID.")
        elif identifier_type == "DATASET_NAME":
            key = dataset_identifier
            if key not in dataset_names:
                raise ValueError(f"Dataset {key} not found in the file.")
        else:
            raise ValueError("Invalid identifier type.")

    df = pd.read_hdf(file_path, key=key)
    headers = df.columns.tolist()
    rows = df.values.tolist()
    return headers, rows

def main():
    parser = argparse.ArgumentParser(description="Fetch data from an HDF5 file.")
    parser.add_argument('--uid', type=str, help='A unique identifier to name the output json file', required=True)

    parser.add_argument('--path', type=str, required=True, help='Path to the file')
    parser.add_argument('--dc_type', type=str, choices=['H5'], required=True, help='Type of the data container')
    parser.add_argument('--h5_dataset_identifier', type=str, required=True, help='Name or index of the dataset')
    parser.add_argument('--h5_identifier_type', type=str, choices=['DATASET_NAME', 'DATASET_ID'], required=True, help='Type of the dataset identifier')

    args = parser.parse_args()

    try:
        headers, rows = read_h5_data(args.path, args.h5_dataset_identifier, args.h5_identifier_type)
    except Exception as e:
        #print(f"Failed to read as normal H5: {e}")
        #print("Attempting to read as PANDAS H5...")
        headers, rows = read_pandas_h5(args.path, args.h5_dataset_identifier, args.h5_identifier_type)

    # Format output
    output = {
        "headers": headers,
        "rows": [[str(item) for item in row] for row in rows],
    }

    #print(json.dumps(output, indent=4))
    json_output = json.dumps(output, indent=4)
    """
    with open('output.json', 'wb') as f:
        # Resize the file to the size of the JSON output
        f.write(b' ' * len(json_output))

    with open('output.json', 'r+b') as f:
        mm = mmap.mmap(f.fileno(), 0)
        mm.write(json_output.encode('utf-8'))
        mm.close()
    """
    filename = f"rgwml_{args.uid}.json"
    with open(filename, 'wb') as f:
        # Resize the file to the size of the JSON output
        f.write(b' ' * len(json_output))

    with open(filename, 'r+b') as f:
        mm = mmap.mmap(f.fileno(), 0)
        mm.write(json_output.encode('utf-8'))
        mm.close()



if __name__ == '__main__':
    main()"#;

/// Python script content for public_url_connect.py
pub const PUBLIC_URL_CONNECT_SCRIPT: &str = r#"import argparse
import json
import pandas as pd
from urllib.parse import urlparse, parse_qs
import mmap

def get_google_sheet_data(sheet_url):
    # Extract the base URL and sheet ID
    parsed_url = urlparse(sheet_url)
    if "spreadsheets/d/" in parsed_url.path:
        base_url = f"https://docs.google.com/spreadsheets/d/{parsed_url.path.split('/d/')[1].split('/')[0]}"
        query_params = parse_qs(parsed_url.fragment)
        gid = query_params.get("gid", ["0"])[0]  # Default to first sheet if gid is not specified
        csv_url = f"{base_url}/export?format=csv&gid={gid}"
    else:
        raise ValueError("Invalid Google Sheet URL format")

    # Read the Google Sheet data into a pandas DataFrame
    df = pd.read_csv(csv_url)

    headers = df.columns.tolist()
    rows = df.values.tolist()

    return headers, rows

def main():
    parser = argparse.ArgumentParser(description="Fetch data from a public Google Sheet.")
    parser.add_argument('--uid', type=str, help='A unique identifier to name the output json file', required=True)
    parser.add_argument('--url', type=str, required=True, help='URL of the Google Sheet')
    parser.add_argument('--url_type', type=str, choices=['GOOGLE_SHEETS'], required=True, help='Type of the URL')

    args = parser.parse_args()

    if args.url_type == 'GOOGLE_SHEETS':
        headers, rows = get_google_sheet_data(args.url)

        # Format output
        output = {
            "headers": headers,
            "rows": [[str(item) for item in row] for row in rows],
        }

        #print(json.dumps(output, indent=4))
        json_output = json.dumps(output, indent=4)
        """
        with open('output.json', 'wb') as f:
            # Resize the file to the size of the JSON output
            f.write(b' ' * len(json_output))

        with open('output.json', 'r+b') as f:
            mm = mmap.mmap(f.fileno(), 0)
            mm.write(json_output.encode('utf-8'))
            mm.close()
        """
        filename = f"rgwml_{args.uid}.json"
        with open(filename, 'wb') as f:
            # Resize the file to the size of the JSON output
            f.write(b' ' * len(json_output))

        with open(filename, 'r+b') as f:
            mm = mmap.mmap(f.fileno(), 0)
            mm.write(json_output.encode('utf-8'))
            mm.close()


    else:
        print("Unsupported URL type")

if __name__ == '__main__':
    main()"#;

/// Python script content for xgb_connect.py
pub const XGB_CONNECT_SCRIPT: &str = r#"import argparse
import dask.dataframe as dd
import dask.array as da
import xgboost as xgb
from dask.distributed import LocalCluster, Client
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import classification_report, confusion_matrix, mean_squared_error, r2_score
import json
import os
import numpy as np
import random
import mmap

def load_data(csv_path):
    df = dd.read_csv(csv_path)
    df = df.fillna('')
    return df

def preprocess_data(df, params_columns, target_column=None, is_test_data=False):
    encoders = {}
    scalers = {}

    def encode_and_scale(df):
        df = df.copy()
        for column in params_columns:
            if df[column].dtype == 'object':
                encoder = LabelEncoder()
                df[column] = encoder.fit_transform(df[column].astype(str))
                encoders[column] = encoder
            else:
                scaler = StandardScaler()
                df[column] = scaler.fit_transform(df[[column]]).flatten()
                scalers[column] = scaler

        if target_column and not is_test_data:
            if df[target_column].dtype == 'object':
                encoder = LabelEncoder()
                df[target_column] = encoder.fit_transform(df[target_column].astype(str))
                encoders[target_column] = encoder

        return df

    meta = {col: 'f8' if df[col].dtype != 'object' else 'i8' for col in df.columns}
    df = df.map_partitions(encode_and_scale, meta=meta)
    return df, encoders, scalers

def split_data(df):
    if 'XGB_TYPE' in df.columns:
        train_df = df[df['XGB_TYPE'] == 'TRAIN']
        validate_df = df[df['XGB_TYPE'] == 'VALIDATE']
        test_df = df[df['XGB_TYPE'] == 'TEST']
    else:
        train_df, test_df = df.random_split([0.8, 0.2], random_state=42)
        validate_df = dd.from_pandas(pd.DataFrame(), npartitions=1)

    return train_df, validate_df, test_df


def train_xgb_model(client, train_df, validate_df, params_columns, target_column, config, is_classification=True):

    config['verbosity'] = 0
    X_train = train_df[params_columns]
    y_train = train_df[target_column]

    d_train = xgb.dask.DaskDMatrix(client, X_train, y_train, enable_categorical=True)

    evals = []
    if len(validate_df) > 0:
        X_validate = validate_df[params_columns]
        y_validate = validate_df[target_column]
        d_validate = xgb.dask.DaskDMatrix(client, X_validate, y_validate, enable_categorical=True)
        evals = [(d_validate, 'validation')]

    model = xgb.dask.train(client, config, d_train, evals=evals)

    return model['booster']



def save_model(model, params_columns, model_dir, model_name):
    os.makedirs(model_dir, exist_ok=True)
    model_path = os.path.join(model_dir, f"{model_name}.json")
    model.save_model(model_path)

def load_model_and_feature_names(model_path):
    model = xgb.Booster()
    model.load_model(model_path)
    feature_names = model.feature_names
    if feature_names is None:
        raise ValueError("Feature names are not found in the model.")
    return model, feature_names

def evaluate_model(client, model, test_df, params_columns, target_column=None, is_classification=True):
    # Ensure test_df contains only test data
    test_data = test_df[test_df['XGB_TYPE'] == 'TEST'].copy()
    X_test = test_data[params_columns]
    y_test = test_data[target_column]
    unique_ids = test_data['unique_id']

    # Use DaskDMatrix for the test data
    d_test = xgb.dask.DaskDMatrix(client, X_test, y_test, enable_categorical=True)
    predictions = xgb.dask.predict(client, model, d_test)
    rounded_predictions = da.round(predictions, 2)

    y_test = y_test.compute()
    predictions = rounded_predictions.compute()

    if is_classification:
        y_test = y_test.astype(int)
        predictions = predictions.astype(int)
        report = classification_report(y_test, predictions, zero_division=0, output_dict=True)
    else:
        y_test = y_test.astype(float)
        predictions = predictions.astype(float)
        mse = mean_squared_error(y_test, predictions)
        r2 = r2_score(y_test, predictions)
        report = {'mean_squared_error': mse, 'r2_score': r2}

    return da.from_array(predictions), unique_ids, report

def invoke_model(client, model, df, params_columns, prediction_column):
    X = df[params_columns]
    dmatrix = xgb.dask.DaskDMatrix(client, X)
    predictions = xgb.dask.predict(client, model, dmatrix)
    rounded_predictions = da.round(predictions, 2)

    return rounded_predictions

def randomize_xgb_config(base_config):
    randomized_config = base_config.copy()
    randomized_config['max_depth'] = random.randint(3, 10)
    randomized_config['learning_rate'] = random.uniform(0.01, 0.3)
    randomized_config['gamma'] = random.uniform(0, 1)
    randomized_config['min_child_weight'] = random.uniform(1, 10)
    randomized_config['subsample'] = random.uniform(0.5, 1.0)
    randomized_config['colsample_bytree'] = random.uniform(0.5, 1.0)
    randomized_config['lambda'] = random.uniform(0, 10)
    randomized_config['alpha'] = random.uniform(0, 10)
    randomized_config['scale_pos_weight'] = random.uniform(0.5, 5.0)
    randomized_config['max_delta_step'] = random.uniform(0, 10)
    return randomized_config

def calculate_deviation(base_config, config):
    deviation = {}
    for key in base_config.keys():
        if isinstance(base_config[key], (int, float)) and isinstance(config[key], (int, float)):
            deviation[key] = config[key] - base_config[key]
        else:
            deviation[key] = "N/A"
    return deviation

def main():
    parser = argparse.ArgumentParser(description='XGBoost Model Training and Prediction Script')
    parser.add_argument('--uid', type=str, help='A unique identifier to name the output json file', required=True)
    parser.add_argument('--csv_path', type=str, help='Path to the CSV file', required=True)
    parser.add_argument('--params', type=str, help='Comma-separated column names to use as parameters for model training', required=True)
    parser.add_argument('--target_column', type=str, help='Name of the target column', required=False)
    parser.add_argument('--prediction_column', type=str, help='Name of the prediction column', required=True)
    parser.add_argument('--model_path', type=str, help='Path to a pre-trained XGBoost model', required=False)
    parser.add_argument('--model_dir', type=str, help='Directory to save the trained model', required=False)
    parser.add_argument('--model_name', type=str, help='Specify the name of the trained model', required=False)
    parser.add_argument('--objective', type=str, default='binary:logistic', help='Objective function')
    parser.add_argument('--max_depth', type=int, default=6, help='Maximum tree depth for base learners')
    parser.add_argument('--learning_rate', type=float, default=0.05, help='Boosting learning rate')
    parser.add_argument('--n_estimators', type=int, default=200, help='Number of boosting rounds')
    parser.add_argument('--gamma', type=float, default=0.0, help='Minimum loss reduction required to make a further partition on a leaf node')
    parser.add_argument('--min_child_weight', type=float, default=1, help='Minimum sum of instance weight (hessian) needed in a child')
    parser.add_argument('--subsample', type=float, default=0.8, help='Subsample ratio of the training instances')
    parser.add_argument('--colsample_bytree', type=float, default=0.8, help='Subsample ratio of columns when constructing each tree')
    parser.add_argument('--reg_lambda', type=float, default=1.0, help='L2 regularization term on weights')
    parser.add_argument('--reg_alpha', type=float, default=0.0, help='L1 regularization term on weights')
    parser.add_argument('--scale_pos_weight', type=float, default=1.0, help='Balancing of positive and negative weights')
    parser.add_argument('--max_delta_step', type=float, default=0.0, help='Maximum delta step we allow each trees weight estimation to be')
    parser.add_argument('--booster', type=str, default='gbtree', help='Which booster to use')
    parser.add_argument('--tree_method', type=str, default='auto', help='Specify the tree construction algorithm used in XGBoost')
    parser.add_argument('--grow_policy', type=str, default='depthwise', help='Controls a way new nodes are added to the tree')
    parser.add_argument('--eval_metric', type=str, default='rmse', help='Evaluation metric for validation data')
    parser.add_argument('--early_stopping_rounds', type=int, default=10, help='Activates early stopping. Validation metric needs to improve at least once in every *early_stopping_rounds* round(s) to continue training')
    parser.add_argument('--device', type=str, default='cpu', help='Device to run the training on (e.g., "cpu", "cuda")')
    parser.add_argument('--cv', type=int, default=5, help='Number of cross-validation folds')
    parser.add_argument('--interaction_constraints', type=str, help='Constraints for interaction between variables')
    parser.add_argument('--hyperparameter_optimization_attempts', type=int, default=0, help='Set to above 0 to activate')
    parser.add_argument('--hyperparameter_optimization_result_display_limit', type=int, default=3, help='Adjust this to change how many rankings of hyperparameter optimizations are returned')
    parser.add_argument('--dask_workers', type=int, default=4, help='Number of dask workers')
    parser.add_argument('--dask_threads_per_worker', type=int, default=1, help='Number of threads per dask worker')

    args = parser.parse_args()
    params_columns = [param.strip() for param in args.params.split(',')]
    prediction_column = args.prediction_column

    base_xgb_config = {
        "objective": args.objective,
        "max_depth": args.max_depth,
        "learning_rate": args.learning_rate,
        "n_estimators": args.n_estimators,
        "gamma": args.gamma,
        "min_child_weight": args.min_child_weight,
        "subsample": args.subsample,
        "colsample_bytree": args.colsample_bytree,
        "lambda": args.reg_lambda,
        "alpha": args.reg_alpha,
        "scale_pos_weight": args.scale_pos_weight,
        "max_delta_step": args.max_delta_step,
        "booster": args.booster,
        "tree_method": args.tree_method,
        "grow_policy": args.grow_policy,
        "eval_metric": args.eval_metric,
        "early_stopping_rounds": args.early_stopping_rounds,
        "device": args.device
    }

    # Convert string arguments to integers
    args.hyperparameter_optimization_attempts = int(args.hyperparameter_optimization_attempts)
    args.dask_workers = int(args.dask_workers)
    args.dask_threads_per_worker = int(args.dask_threads_per_worker)


    with LocalCluster(n_workers=args.dask_workers, threads_per_worker=args.dask_threads_per_worker) as cluster:
        with cluster.get_client() as client:
            df = load_data(args.csv_path)
            df = df.reset_index(drop=True).persist()
            df['unique_id'] = df.index
            df = df.persist()


            if args.model_path:
                model, feature_names = load_model_and_feature_names(args.model_path)
                if set(params_columns) != set(feature_names):
                    raise ValueError(f"Feature names mismatch: expected {feature_names} but got {params_columns}")
                df_preprocessed, encoders, scalers = preprocess_data(df, params_columns, is_test_data=True)
                predictions = invoke_model(client, model, df_preprocessed, params_columns, args.prediction_column)
                df[args.prediction_column] = predictions
                df = df.drop(columns=['unique_id'])

                df_computed = df.compute()
                # Extract headers and rows for output
                headers = list(df_computed.columns)
                rows = df_computed.astype(str).values.tolist()

                # Prepare output
                output = {
                    "headers": headers,
                    "rows": rows,
                }

            else:

                df_preprocessed, encoders, scalers = preprocess_data(df, params_columns, args.target_column)
                train_df, validate_df, test_df = split_data(df_preprocessed)
                train_df = train_df.reset_index(drop=True).persist()
                validate_df = validate_df.reset_index(drop=True).persist()
                test_df = test_df.reset_index(drop=True).persist()

                is_classification = "binary" in args.objective or "multi" in args.objective
                if args.hyperparameter_optimization_attempts == 0:
                    model = train_xgb_model(client, train_df, validate_df, params_columns, args.target_column, base_xgb_config, is_classification)
                    save_model(model, params_columns, args.model_dir, args.model_name)
                    # Evaluate the model
                    predictions, unique_ids, report = evaluate_model(client, model, test_df, params_columns, args.target_column, is_classification)
                else:
                    best_score = float('inf') if "reg:" in args.objective else 0
                    best_config = base_xgb_config
                    trials = []

                    for attempt in range(args.hyperparameter_optimization_attempts):
                        if attempt == 0:
                            xgb_config = base_xgb_config
                        else:
                            xgb_config = randomize_xgb_config(base_xgb_config)
                        
                        model = train_xgb_model(client, train_df, validate_df, params_columns, args.target_column, xgb_config, is_classification)
                        predictions, unique_ids, report = evaluate_model(client, model, test_df, params_columns, args.target_column, is_classification)

                        if "reg:" in args.objective:
                            trials.append({
                                "params": xgb_config, 
                                "mean_squared_error": report['mean_squared_error'], 
                                "r2_score": report['r2_score'],
                                "deviation_from_base": calculate_deviation(base_xgb_config, xgb_config)
                            })
                            score = report['mean_squared_error']
                        else:
                            trials.append({
                                "params": xgb_config, 
                                "accuracy": report['accuracy'],
                                "macro_avg": report['macro avg'],
                                "weighted_avg": report['weighted avg'],
                                "deviation_from_base": calculate_deviation(base_xgb_config, xgb_config)
                            })
                            score = report['accuracy']

                        if ("reg:" in args.objective and score < best_score) or ("reg:" not in args.objective and score > best_score):
                            best_score = score
                            best_config = xgb_config

                    xgb_config = best_config
                    model = train_xgb_model(client, train_df, validate_df, params_columns, args.target_column, xgb_config, is_classification)
                    save_model(model, params_columns, args.model_dir, args.model_name)
                    predictions, unique_ids, report = evaluate_model(client, model, test_df, params_columns, args.target_column, is_classification)

                    if "reg:" in args.objective:
                        sorted_trials = sorted(trials, key=lambda x: x['mean_squared_error'], reverse="reg:" not in args.objective)
                    else:
                        sorted_trials = sorted(trials, key=lambda x: x['accuracy'], reverse="reg:" not in args.objective)

                    num_attempts = len(sorted_trials)
                    report["best_params"] = [{f"rank_{i+1}": sorted_trials[i]} for i in range(num_attempts)]
                    report["best_params"] = report["best_params"][:args.hyperparameter_optimization_result_display_limit]
                    report["best_params"] = report["best_params"][::-1]

                    if "reg:" in args.objective:
                        report["best_mean_squared_error"] = report["mean_squared_error"]
                        report["best_r2_score"] = report["r2_score"]

                        # Remove the old keys
                        del report["mean_squared_error"]
                        del report["r2_score"]
                    else:
                        report["best_rank_stats"] = {
                                "best_accuracy": report["accuracy"],
                                "best_macro_avg": report["macro avg"],
                                "best_weighted_avg": report["weighted avg"],
                            }
                        del report["accuracy"]
                        del report["macro avg"]
                        del report["weighted avg"]
                        del report["0"]
                        del report["1"]



                    predictions_df = dd.from_dask_array(predictions, columns=[prediction_column])
                    predictions_df['unique_id'] = unique_ids
                    predictions_df['unique_id'] = predictions_df['unique_id'].astype('int64')
                    final_df = df.merge(predictions_df, on='unique_id', how='left')
                    final_df = final_df.drop(columns=['unique_id'])
                    final_df[prediction_column] = final_df[prediction_column].round(2)
                    final_df[prediction_column] = final_df[prediction_column].astype('str')
                    final_df[prediction_column] = final_df[prediction_column].replace('nan', '')
                    results = final_df.compute().values.tolist()
                    headers = final_df.columns.tolist()
                    output = {"headers": headers, "rows": [[str(item) for item in row] for row in results], "report": report}



                # STEP 2: Once the predictions are in along with their corresponding unique ids, you can add it back to the original df
                # Create a new Dask DataFrame for predictions with unique_id
                predictions_df = dd.from_dask_array(predictions, columns=[prediction_column])
                predictions_df['unique_id'] = unique_ids
                predictions_df['unique_id'] = predictions_df['unique_id'].astype('int64')

                # Merge predictions into the original test DataFrame
                final_df = df.merge(predictions_df, on='unique_id', how='left')
                final_df = final_df.drop(columns=['unique_id'])
                final_df[prediction_column] = final_df[prediction_column].round(2)
                final_df[prediction_column] = final_df[prediction_column].astype('str')
                final_df[prediction_column] = final_df[prediction_column].replace('nan', '')


                # Compute results
                results = final_df.compute().values.tolist()
                headers = final_df.columns.tolist()

                output = {
                    "headers": headers,
                    "rows": [[str(item) for item in row] for row in results],
                    "report": report
                }

            #print("IGNORE_POINT")
            #print(json.dumps(output, indent=4))
            json_output = json.dumps(output, indent=4)
            filename = f"rgwml_{args.uid}.json"
            with open(filename, 'wb') as f:
                # Resize the file to the size of the JSON output
                f.write(b' ' * len(json_output))

            with open(filename, 'r+b') as f:
                mm = mmap.mmap(f.fileno(), 0)
                mm.write(json_output.encode('utf-8'))
                mm.close()



if __name__ == "__main__":
    main()"#;
